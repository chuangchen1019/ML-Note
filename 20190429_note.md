# Reinforcement Learning

> <span style='color:#FF6369'> **Agent -> action -> reward or punishment**</span>
>
> maximizing expected award (讓agent滿足我們的期望)

------

<span style = 'color:#7dd389'>**Action = function(Observation)**</span>

```c
Input = observation

output = action
```

ex : AlphaGo -> 圍棋 ( input=棋盤的落子現況, reward = 1 or -1(win or lost) )

> **Supervised Learning**
>
> -> learn from teacher
>
> **Reinforcement Learning**
>
> -> learn from experience

Learning a ChatBot -> let two agents talk to each other

------

<span style = 'color:#7dd389'>**Properties of Reinforcement Learning**</span>

* Reward delay
* every action is dependent (現在的action會影響以後)

------



# Model-free approach

-> policy-based  (learn as an actor)

-> value-based  (learn as an critic)

ALpha GO = policy-based + value-based + mode-based

------

<span style = 'color:#7dd389'>**Policy-based Model**</span>

neural network as actor

```c
Input = observation(pixels)

Output = probability of taking the action
```

generalization (不用管圖片有沒有看過 都可以算出機率)

> 1. define function set
> 2. define goodness function
> 3. pick the best function

loss -> crossentropy -> sum(- yhead lny) 

...

Given an actor pitheta(s)

Use tje actor pitheta(s) to play game

Total reword
$$
R\theta = \Sigma  Rt
$$

> R theta high -> great at play games 
>
> s -> observation (status)
>
> a -> action
>
> r -> rewardpi -> neural network structure
>
> Theta -> network parameter setting 

* <span style = 'color:#2f79d1'>p(at|st,theta) : 在看到畫面的情況下發生某action的機率為...</span>

  ​	ex. p(at = 'fire'|game) = 0.7 (在這個畫面下fire的機率是0.7)

* Episode is considered as trajectory (玩一次遊戲 = 1episode)

* 一次玩出來的分數是R(tao) -> 全部加總等於期望值

* Arg : 取當時的parameter setting

